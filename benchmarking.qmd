---
title: "Expression Template Benchmarking"
author: Marc Paterno
date: last-modified
execute: 
  echo: false
format:
  html:
    number-sections: true
    margin-left: 100px
    margin-right: 50px
  pdf:
    number-sections: true
    pdf-engine: xelatex
---

```{r}
#| include: false
library(ggplot2)
source("functions.R")
```

## Purpose of this document

This document explores the code generation and performance of expression templates in the context of [*dual numbers*](https://en.wikipedia.org/wiki/Dual_number).
In particular, we are concerned with artithmetic operations on dual numbers that occur in the field of automatic differentiation.

## Benchmarking results

We have collected performance data on an MacBook Pro laptop with ann Intel i9 core chip.
We used the [nanobench](https://github.com/martinus/nanobench) library to collect performance information.

Our collected data have the following form:

```{r}
bm <- read_benchmark_data("timing.txt")
knitr::kable(head(bm, 10))
```
Each benchmark execution was repeated 10 times in the data shown below.

@fig-each-compiler shows a comparison of the different functions with results grouped by compiler.
Note that each plot has a different scale on the $x$ axis.

```{r}
#| fig-asp: 0.667
#| out-width: "100%"
#| fig-cap: "Comparison between different functions for each compiler."
#| label: fig-each-compiler
ggplot(bm, aes(time, func)) +
  geom_boxplot() +
  facet_wrap(vars(compiler), nrow = 2, scales = "free_x") +
  labs(x="execution time (ns)", y="")
```

We will compare first the measured speed of the *constructor* call to the other functions.
The code measured by *constructor* is shown in @lst-ctor.
This is intended to be as similar as possible to the code used in the benchmarking function used for the function we wish to compare.
@lst-ctor-benchmark shows the actual benchmark executed.
The function call differs from that used for benchmarking the various multiplication functions in that it takes 6 `double`s by value, rather than 3 `DualNum`s by `const` reference.

```{#lst-ctor .cpp lst-cap="The `only_construct` function used in benchmarking."}

DualNum5
only_construct(double a, double b, double c, double d, double e, double f)
{
  // Return uses DualNum5 constructor using 6 doubles
  return {a, b, c, d, e, f};
}
```


```{#lst-ctor-benchmark .cpp lst-cap="Benchmark function for timing the `DualNum5` constructor."}
void
run_bench_constructor(ankerl::nanobench::Bench* bench, char const* name)
{
  ankerl::nanobench::Rng rng(137);
  auto                   gen = [&]() { return rng.uniform01(); };
  double                 a   = gen();
  double                 b   = gen();
  double                 c   = gen();
  double                 d   = gen();
  double                 e   = gen();
  double                 f   = gen();
  bench->run(name, [&]() {
    DualNum5 z = only_construct(a, b, c, d, e, f);
    ankerl::nanobench::doNotOptimizeAway(z);
  });
}
```

It appears that GCC 10 does a much worse job than the other compilers of optimizing the construction time.
Perhaps inspecting the assembly language code will explain why.
@lst-gcc10-ctor-asm and @list-gcc11-ctor-asm show the assembly language generated for the same C++ source code by GCC 10 and GCC 11, the worst- and best-performing of the GCC implementations.
The code difference is not large, although the performance difference is quite significant.

::::{.columns .column-body-outset}
:::{.column width=50%}
```{#lst-gcc10-ctor-assm .asm lst-cap="Optimized assembly code output from GCC 10 for the `only_construct` function."}
vunpcklpd       %xmm3, %xmm2, %xmm2
vunpcklpd       %xmm1, %xmm0, %xmm1
movq    %rdi, %rax
vinsertf128     $0x1, %xmm2, %ymm1, %ymm0
vmovupd %ymm0, (%rdi)
vmovsd  %xmm4, 32(%rdi)
vmovsd  %xmm5, 40(%rdi)
vzeroupper
ret
```
:::
::: {.column width=50%}
```{#lst-gcc11-ctor-assm .asm lst-cap="Optimized assembly code output from GCC 11 for the `only_construct` function."}
vunpcklpd       %xmm3, %xmm2, %xmm2
vunpcklpd       %xmm1, %xmm0, %xmm1
movq    %rdi, %rax
vinsertf128     $0x1, %xmm2, %ymm1, %ymm0
vunpcklpd       %xmm5, %xmm4, %xmm4
vmovupd %ymm0, (%rdi)
vmovupd %xmm4, 32(%rdi)
vzeroupper
ret
```
:::
::::

Next we look at the same data organized to allow easy comparison for each function across the different compilers.

```{r}
#| fig-asp: 0.333
#| out-width: "100%"
#| fig-cap: "Comparisons between different compilers for each function."
#| label: fig-each-function
ggplot(bm, aes(time, compiler)) +
  geom_boxplot() +
  facet_wrap(vars(func), nrow = 1, scales = "free_x")
```


We can subtract the construction time from both of the other calculations to infer how much time is actually spent in the calculations.
Since we do not have the time broken down by individual run we calculate means, and subtract them.

```{r}
means <- make_means(bm)
```

I am not yet sure what to make of @fig-scatter.

```{r}
#| out-width: "100%"
#| fig-cap: "Comparison of calculation times by compiler."
#| label: fig-scatter
make_for_scatterplot(means) |>
  ggplot(aes(x=x,xmin=xmin,xmax=xmax,y=y,ymin=ymin,ymax=ymax, color=compiler)) +
  geom_errorbar(width=0.0) +
  geom_errorbarh(height=0.0) +  
  geom_point() +
  labs(x = "template time (ns)", y = "handopt time (ns)")
```
