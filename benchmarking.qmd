---
title: "Expression Template Benchmarking"
author: Marc Paterno
date: last-modified
execute: 
  echo: false
format:
  html:
    number-sections: true
    margin-left: 100px
    margin-right: 50px
  pdf:
    number-sections: true
    pdf-engine: xelatex
---

```{r}
#| include: false
library(ggplot2)
source("functions.R")
```

## Purpose of this document

This document explores the code generation and performance of expression templates in the context of [*dual numbers*](https://en.wikipedia.org/wiki/Dual_number).
In particular, we are concerned with artithmetic operations on dual numbers that occur in the field of automatic differentiation.

## Benchmarking results

We have collected performance data on an MacBook Pro laptop with ann Intel i9 core chip.
We used the [nanobench](https://github.com/martinus/nanobench) library to collect performance information.

Our collected data have the following form:

```{r}
bm <- read_benchmark_data("timing.txt")
knitr::kable(head(bm, 10))
```
Each benchmark execution was repeated 10 times in the data shown below.

@fig-each-compiler shows a comparison of the different functions with results grouped by compiler.
Note that each plot has a different scale on the $x$ axis.

```{r}
#| fig-asp: 0.667
#| out-width: "100%"
#| fig-cap: "Comparison between different functions for each compiler."
#| label: fig-each-compiler
ggplot(bm, aes(time, func)) +
  geom_boxplot() +
  facet_wrap(vars(compiler), nrow = 2, scales = "free_x") +
  labs(x="execution time (ns)", y="")
```

### Focus on the construction

C++ provides for guaranteed copy elision for some [value categories](https://en.cppreference.com/w/cpp/language/value_category).
In particular: if a function returns a non-reference type and a unique object constructed within the function is returned by all return paths (and subject to a few technical restrictions) the space for that object is actually allocated in the calling code and the object in the function is constructed in the space allocated in the calling code.
Under these circumstancees returning an object by value is as efficient, and sometimes more efficient (because the compiler can verify that reference aliasing is not taking place) than return by reference through an "out parameter".

@fig-each-compiler indicates that the functions we are benchmarking are all, in an absolute sense, fast; none takes more than a few nanoseconds.
The execution time of the functions labeled `constructor` in @fig-each-compiler is, for each compiler, shorter than the time taken by any of the other functions.
We intend this time to be an estimate of the time taken to construct and return the `DualNum5` object; given the guaranteed copy elision, this means the time taken to set the state of the memory in the caller to reflect the correct value of the product of the three `DualNum5` objects calculated by the functions we are profiling.
To infer the time taken by the calculation of the return value, as opposed to the time taken by setting the state of the return value, we propose to subtract the time taken by the `only_construct` function from the time taken by the various multiplication functions.
Before we trust that this is appropriate we will look in some detail at the `only_construct` function, to make sure we understand the behavior and performance we are observing.

@lst-ctor shows the C++ code of the `only_construct` function.
@fig-only-construct shows the timing data observed for 10 runs each of this function.

```{r}
#| fig-asp: 0.5
#| out-width: 100%
#| fig-cap: "Comparison of the `only_construct` function timing for all compilers."
#| label: fig-only-construct
dplyr::filter(bm, func == "constructor") |>
  ggplot(aes(time, reorder(compiler, time))) +
  geom_boxplot() +
  scale_x_log10() +
  labs(x = "execution time (ns)", y = "compiler")
```

Each compiler produces quite reproducible results.
The two clang compilers produce statistically indistinguishable results.
GCC 11 and 12 also produce statistically indistinguishable results, but those results are slightly, but measurably, slower than the two clang compilers.
The most striking feature is how much slower is the code generated by GCC 10 than the code generated by the other compilers.

We will compare first the measured speed of the *constructor* call to the other functions.
The code measured by *constructor* is shown in @lst-ctor.
This is intended to be as similar as possible to the code used in the benchmarking function used for the function we wish to compare.
@lst-ctor-benchmark shows the actual benchmark executed.
The function call differs from that used for benchmarking the various multiplication functions in that it takes 6 `double`s by value, rather than 3 `DualNum`s by `const` reference.

```{#lst-ctor .cpp lst-cap="The `only_construct` function used in benchmarking."}

DualNum5
only_construct(double a, double b, double c, double d, double e, double f)
{
  // Return uses DualNum5 constructor using 6 doubles
  return {a, b, c, d, e, f};
}
```


```{#lst-ctor-benchmark .cpp lst-cap="Benchmark function for timing the `DualNum5` constructor."}
void
run_bench_constructor(ankerl::nanobench::Bench* bench, char const* name)
{
  ankerl::nanobench::Rng rng(137);
  auto                   gen = [&]() { return rng.uniform01(); };
  double                 a   = gen();
  double                 b   = gen();
  double                 c   = gen();
  double                 d   = gen();
  double                 e   = gen();
  double                 f   = gen();
  bench->run(name, [&]() {
    DualNum5 z = only_construct(a, b, c, d, e, f);
    ankerl::nanobench::doNotOptimizeAway(z);
  });
}
```

It appears that GCC 10 does a much worse job than the other compilers of optimizing the construction time.
Perhaps inspecting the assembly language code will explain why.
@lst-gcc10-ctor-asm and @lst-gcc11-ctor-asm show the assembly language generated for the same C++ source code by GCC 10 and GCC 11, the worst- and best-performing of the GCC implementations.
The code difference is not large, although the performance difference is quite significant.

::::{.columns .column-body-outset}
:::{.column width=50%}
```{#lst-gcc10-ctor-asm .asm lst-cap="Optimized assembly code output from GCC 10 for the `only_construct` function."}
vunpcklpd       %xmm3, %xmm2, %xmm2
vunpcklpd       %xmm1, %xmm0, %xmm1
movq    %rdi, %rax
vinsertf128     $0x1, %xmm2, %ymm1, %ymm0
vmovupd %ymm0, (%rdi)
vmovsd  %xmm4, 32(%rdi)
vmovsd  %xmm5, 40(%rdi)
vzeroupper
ret
```
:::
::: {.column width=50%}
```{#lst-gcc11-ctor-asm .asm lst-cap="Optimized assembly code output from GCC 11 for the `only_construct` function."}
vunpcklpd       %xmm3, %xmm2, %xmm2
vunpcklpd       %xmm1, %xmm0, %xmm1
movq    %rdi, %rax
vinsertf128     $0x1, %xmm2, %ymm1, %ymm0
vunpcklpd       %xmm5, %xmm4, %xmm4
vmovupd %ymm0, (%rdi)
vmovupd %xmm4, 32(%rdi)
vzeroupper
ret
```
:::
::::

It is possible that the speed difference betwen the benchmarks above is due to the context in which the `only_construct` function is called.
To determine this we can look at the assembler output for the `run_bench_constructor` calls.
The full code is large, so we concentrate on the portion of the code that is actually timed.
@lst-gcc-ctor-bm-asm shows the assembly language generated for the same C++ source code by GCC 10 and GCC 11; @


```{#lst-gcc-ctor-bm-asm .asm lst-cap="Optimized assembly code output from GCC 10 and GCC 11 for the `run_benchmark_constructor` function."}
	call	ankerl::nanobench::detail::PerformanceCounters::beginMeasure()
	call	std::chrono::_V2::steady_clock::now()
	movq	%rax, %r14
	decq	%rbp
	.p2align 4,,10
	.p2align 3
L17:
	movq	32(%rbx), %rdi
	movq	40(%rbx), %r8
	movq	24(%rbx), %rsi
	movq	16(%rbx), %rcx
	movq	8(%rbx), %rdx
	movq	(%rbx), %rax
	vmovsd	(%rdi), %xmm4
	vmovsd	(%r8), %xmm5
	vmovsd	(%rsi), %xmm3
	vmovsd	(%rcx), %xmm2
	vmovsd	(%rdx), %xmm1
	vmovsd	(%rax), %xmm0
	movq	%r12, %rdi
	call	only_construct(double, double, double, double, double, double)
	vmovdqu	32(%rsp), %xmm6
	vmovdqu	48(%rsp), %xmm7
	vmovdqu	64(%rsp), %xmm0
	vmovdqu	%xmm6, 80(%rsp)
	vmovdqu	%xmm7, 96(%rsp)
	vmovdqu	%xmm0, 112(%rsp)
	subq	$1, %rbp
	jnb	L17
	call	std::chrono::_V2::steady_clock::now()
	movq	%r13, %rdi
	movq	%rax, %rbp
	call	ankerl::nanobench::detail::PerformanceCounters::endMeasure()
```

Next we look at the same data organized to allow easy comparison for each function across the different compilers.

```{r}
#| fig-asp: 0.333
#| out-width: "100%"
#| fig-cap: "Comparisons between different compilers for each function."
#| label: fig-each-function
ggplot(bm, aes(time, compiler)) +
  geom_boxplot() +
  facet_wrap(vars(func), nrow = 1, scales = "free_x")
```


We can subtract the construction time from both of the other calculations to infer how much time is actually spent in the calculations.
Since we do not have the time broken down by individual run we calculate means, and subtract them.

```{r}
means <- make_means(bm)
```

I am not yet sure what to make of @fig-scatter.

```{r}
#| out-width: "100%"
#| fig-cap: "Comparison of calculation times by compiler."
#| label: fig-scatter
make_for_scatterplot(means) |>
  ggplot(aes(x=x,xmin=xmin,xmax=xmax,y=y,ymin=ymin,ymax=ymax, color=compiler)) +
  geom_errorbar(width=0.0) +
  geom_errorbarh(height=0.0) +  
  geom_point() +
  labs(x = "template time (ns)", y = "handopt time (ns)")
```
